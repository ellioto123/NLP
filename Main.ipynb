{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ellio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ellio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from DataSplit import load_data\n",
    "from Tokenizer import tokenize\n",
    "\n",
    "X_train, X_eval, X_test, y_train, y_eval, y_test = load_data()\n",
    "\n",
    "\n",
    "tokenize(X_train) # Basic tokenizer that removes punctuation and lowercases everything\n",
    "tokenize(X_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('sure', 'JJ'), ('cheesy', 'VB'), ('nonsensical', 'JJ'), ('time', 'NN'), ('corny', 'NN'), ('least', 'JJS'), ('filmmaker', 'NN'), ('didnt', 'NN'), ('try', 'VB'), ('tv', 'NN'), ('movie', 'NN'), ('border', 'NN'), ('brink', 'NN'), ('mediocrity', 'NN'), ('film', 'NN'), ('actually', 'RB'), ('redeeming', 'VBG'), ('quality', 'NN'), ('cinematography', 'NN'), ('pretty', 'RB'), ('good', 'JJ'), ('tv', 'NN'), ('film', 'NN'), ('viggo', 'NN'), ('mortensen', 'NN'), ('display', 'NN'), ('shade', 'VBD'), ('aragorn', 'JJ'), ('film', 'NN'), ('man', 'NN'), ('played', 'VBD'), ('rule', 'NN'), ('flashback', 'NN'), ('sequence', 'NN'), ('kind', 'NN'), ('cheesy', 'JJ'), ('scene', 'NN'), ('mountain', 'NN'), ('lion', 'NN'), ('intense', 'JJ'), ('kind', 'NN'), ('annoyed', 'VBD'), ('jason', 'NN'), ('priestlys', 'NN'), ('role', 'NN'), ('film', 'NN'), ('rebellious', 'JJ'), ('shockjock', 'NN'), ('tv', 'NN'), ('movie', 'NN'), ('despite', 'IN'), ('good', 'JJ'), ('thing', 'NN'), ('soundtrack', 'NN'), ('atrocious', 'JJ'), ('however', 'RB'), ('nice', 'JJ'), ('see', 'VBP'), ('tucson', 'JJ'), ('arizona', 'NNS'), ('prominently', 'RB'), ('featured', 'VBD'), ('film', 'NN')]\n",
      "['nonsensical time corny', 'filmmaker didnt', 'tv movie border brink mediocrity film', 'quality cinematography', 'good tv film viggo mortensen display', 'aragorn film man', 'rule flashback sequence kind', 'cheesy scene mountain lion', 'intense kind', 'jason priestlys role film', 'rebellious shockjock tv movie', 'good thing soundtrack', 'film']\n",
      "[('movie', 'NN'), ('could', 'MD'), ('much', 'RB'), ('better', 'RBR'), ('script', 'NN'), ('rewrite', 'NN'), ('not', 'RB'), ('expect', 'VB'), ('great', 'JJ'), ('deal', 'JJ'), ('plausibility', 'NN'), ('movie', 'NN'), ('youd', 'NN'), ('think', 'VBP'), ('even', 'RB'), ('homeless', 'JJ'), ('urbandwelling', 'VBG'), ('jack', 'NN'), ('mason', 'NN'), ('would', 'MD'), ('question', 'VB'), ('group', 'NN'), ('experienced', 'JJ'), ('hunter', 'NN'), ('would', 'MD'), ('want', 'VB'), ('hire', 'NN'), ('hunting', 'VBG'), ('guide', 'JJ'), ('upon', 'IN'), ('reaching', 'VBG'), ('hunting', 'VBG'), ('ground', 'NN'), ('poor', 'JJ'), ('icet', 'NNS'), ('play', 'VBP'), ('part', 'NN'), ('actually', 'RB'), ('going', 'VBG'), ('lead', 'JJ'), ('men', 'NNS'), ('wood', 'VBD'), ('he', 'PRP'), ('never', 'RB'), ('seen', 'VBN'), ('beforebr', 'NN'), ('br', 'NN'), ('jack', 'NN'), ('mason', 'NN'), ('find', 'VBP'), ('thomas', 'JJ'), ('burn', 'VBP'), ('back', 'RP'), ('seattlebr', 'JJ'), ('br', 'NN'), ('im', 'NN'), ('assuming', 'VBG'), ('movie', 'NN'), ('based', 'VBN'), ('richard', 'NN'), ('connells', 'NNS'), ('short', 'JJ'), ('story', 'NN'), ('dangerous', 'JJ'), ('game', 'NN'), ('year', 'NN'), ('ago', 'RB'), ('showed', 'VBD'), ('movie', 'NN'), ('class', 'NN'), ('9th', 'CD'), ('grade', 'JJ'), ('student', 'NN'), ('read', 'VBD'), ('story', 'NN'), ('reedited', 'VBN'), ('movie', 'NN'), ('cutting', 'VBG'), ('pointless', 'JJ'), ('scene', 'JJ'), ('profanity', 'NN'), ('ended', 'VBD'), ('43', 'CD'), ('minute', 'NN'), ('long', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "from Tokenizer import remove_stopwords_and_stem\n",
    "from Tokenizer import remove_stopwords_and_lemmatize\n",
    "from FrequencyCut import cut\n",
    "from PoSTagging import run\n",
    "import numpy as np\n",
    "\n",
    "# edit the stopwords to not remove the word \"not\"\n",
    "\n",
    "for x in range(len(X_train)):\n",
    "    # X_train[x] = remove_stopwords_and_stem(X_train[x])\n",
    "    X_train[x] = remove_stopwords_and_lemmatize(X_train[x])\n",
    "\n",
    "for x in range(len(X_eval)):\n",
    "    # X_eval[x] = remove_stopwords_and_stem(X_eval[x])\n",
    "    X_eval[x] = remove_stopwords_and_lemmatize(X_eval[x])\n",
    "\n",
    "\n",
    "# get all the words in the training set\n",
    "\n",
    "# X_train = cut(X_train)\n",
    "# X_eval = cut(X_eval)\n",
    "\n",
    "nounPhrasestrain = run(X_train)\n",
    "print(nounPhrasestrain[0])\n",
    "# min = np.min(postagstrain)\n",
    "# max = np.max(postagstrain)\n",
    "# postagstrain = (postagstrain - min) / (max - min) #normalise\n",
    "\n",
    "nounPhraseseval = run(X_eval)\n",
    "\n",
    "# min = np.min(postagseval)\n",
    "# max = np.max(postagseval)\n",
    "# postagseval = (postagseval - min) / (max - min) #normalise\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('im', 'sure'), ('sure', 'folk'), ('folk', 'texaslouisiana'), ('texaslouisiana', 'border'), ('border', 'must'), ('must', 'good'), ('good', 'laugh'), ('laugh', 'two'), ('two', 'paramounts'), ('paramounts', 'b'), ('b', 'picture'), ('picture', 'unit'), ('unit', 'inflicted'), ('inflicted', 'one'), ('one', 'war'), ('war', 'time'), ('time', 'public'), ('public', 'simply'), ('simply', 'area'), ('area', 'along'), ('along', 'sabine'), ('sabine', 'river'), ('river', 'film'), ('film', 'open'), ('open', 'cotton'), ('cotton', 'country'), ('country', 'like'), ('like', 'rest'), ('rest', 'deep'), ('deep', 'south'), ('south', 'least'), ('least', 'deep'), ('deep', 'south'), ('south', 'post'), ('post', 'civl'), ('civl', 'war'), ('war', 'big'), ('big', 'cattle'), ('cattle', 'empire'), ('empire', 'theyre'), ('theyre', 'much'), ('much', 'farther'), ('farther', 'west'), ('west', 'texas'), ('texas', 'farther'), ('farther', 'richard'), ('richard', 'dix'), ('dix', 'preston'), ('preston', 'foster'), ('foster', 'could'), ('could', 'ride'), ('ride', 'set'), ('set', 'empirebr'), ('empirebr', 'br'), ('br', 'film'), ('film', 'begin'), ('begin', 'two'), ('two', 'partner'), ('partner', 'riverboat'), ('riverboat', 'leo'), ('leo', 'carrillo'), ('carrillo', 'try'), ('try', 'theft'), ('theft', 'service'), ('service', 'not'), ('not', 'paying'), ('paying', 'hauling'), ('hauling', 'cattle'), ('cattle', 'keep'), ('keep', 'cattle'), ('cattle', 'thats'), ('thats', 'beginning'), ('beginning', 'big'), ('big', 'ponderosa'), ('ponderosa', 'like'), ('like', 'ranch'), ('ranch', 'startbr'), ('startbr', 'br'), ('br', 'along'), ('along', 'way'), ('way', 'foster'), ('foster', 'marries'), ('marries', 'dix'), ('dix', 'sister'), ('sister', 'played'), ('played', 'france'), ('france', 'gifford'), ('gifford', 'feud'), ('feud', 'much'), ('much', 'smaller'), ('smaller', 'neighbor'), ('neighbor', 'also'), ('also', 'run'), ('run', 'in'), ('in', 'leo'), ('leo', 'carrillobr'), ('carrillobr', 'br'), ('br', 'anyway'), ('anyway', 'u'), ('u', 'easterner'), ('easterner', 'like'), ('like', 'western'), ('western', 'usually'), ('usually', 'dont'), ('dont', 'bother'), ('bother', 'geographical'), ('geographical', 'trifle'), ('trifle', 'still'), ('still', 'good'), ('good', 'western'), ('western', 'production'), ('production', 'mill'), ('mill', 'harry'), ('harry', 'sherman'), ('sherman', 'produced'), ('produced', 'hopalong'), ('hopalong', 'cassidy'), ('cassidy', 'western'), ('western', 'paramount'), ('paramount', 'climax'), ('climax', 'blazing'), ('blazing', 'mean'), ('mean', 'literally'), ('literally', 'gun'), ('gun', 'battle'), ('battle', 'maybe'), ('maybe', 'used'), ('used', 'productionbr'), ('productionbr', 'br'), ('br', 'wouldnt'), ('wouldnt', 'western'), ('western', 'fan'), ('fan', 'look')]\n"
     ]
    }
   ],
   "source": [
    "#Generate n-grams\n",
    "from NGrams import generate_ngrams\n",
    "n = 2\n",
    "X_train_ngrams = generate_ngrams(X_train, n)\n",
    "X_eval_ngrams = generate_ngrams(X_eval, n)\n",
    "\n",
    "print (X_train_ngrams[1])\n",
    "\n",
    "\n",
    "# Print the n-grams\n",
    "# for i, ngrams in enumerate(X_train_ngrams):\n",
    "#     print(f\"Sentence {i+1} {n}-grams: {ngrams}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done1\n",
      "done2\n",
      "done5\n",
      "done6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>\\tas</th>\n",
       "      <th>\\tfirst</th>\n",
       "      <th>\\tjason</th>\n",
       "      <th>\\tmaking</th>\n",
       "      <th>\\tthe</th>\n",
       "      <th>music play</th>\n",
       "      <th>accurate</th>\n",
       "      <th>action adventure success</th>\n",
       "      <th>admission</th>\n",
       "      <th>...</th>\n",
       "      <th>which</th>\n",
       "      <th>à</th>\n",
       "      <th>álex</th>\n",
       "      <th>álvaro</th>\n",
       "      <th>ángel</th>\n",
       "      <th>åge</th>\n",
       "      <th>écran</th>\n",
       "      <th>émigré</th>\n",
       "      <th>émigrés</th>\n",
       "      <th>østbye</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.457351</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.491470</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 105200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               \\tas  \\tfirst  \\tjason  \\tmaking  \\tthe    music play  \\\n",
       "0    0.000000     0        0        0         0    0.0             0   \n",
       "1    0.000000     0        0        0         0    0.0             0   \n",
       "2    0.000000     0        0        0         0    0.0             0   \n",
       "3    0.000000     0        0        0         0    0.0             0   \n",
       "4    2.457351     0        0        0         0    0.0             0   \n",
       "..        ...   ...      ...      ...       ...    ...           ...   \n",
       "395  0.000000     0        0        0         0    0.0             0   \n",
       "396  0.000000     0        0        0         0    0.0             0   \n",
       "397  0.491470     0        0        0         0    0.0             0   \n",
       "398  0.000000     0        0        0         0    0.0             0   \n",
       "399  0.000000     0        0        0         0    0.0             0   \n",
       "\n",
       "      accurate   action adventure success   admission  ...   which  à  álex  \\\n",
       "0            0                          0           0  ...       0  0     0   \n",
       "1            0                          0           0  ...       0  0     0   \n",
       "2            0                          0           0  ...       0  0     0   \n",
       "3            0                          0           0  ...       0  0     0   \n",
       "4            0                          0           0  ...       0  0     0   \n",
       "..         ...                        ...         ...  ...     ... ..   ...   \n",
       "395          0                          0           0  ...       0  0     0   \n",
       "396          0                          0           0  ...       0  0     0   \n",
       "397          0                          0           0  ...       0  0     0   \n",
       "398          0                          0           0  ...       0  0     0   \n",
       "399          0                          0           0  ...       0  0     0   \n",
       "\n",
       "     álvaro  ángel  åge  écran  émigré  émigrés  østbye  \n",
       "0         0      0    0      0       0        0       0  \n",
       "1         0      0    0      0       0        0       0  \n",
       "2         0      0    0      0       0        0       0  \n",
       "3         0      0    0      0       0        0       0  \n",
       "4         0      0    0      0       0        0       0  \n",
       "..      ...    ...  ...    ...     ...      ...     ...  \n",
       "395       0      0    0      0       0        0       0  \n",
       "396       0      0    0      0       0        0       0  \n",
       "397       0      0    0      0       0        0       0  \n",
       "398       0      0    0      0       0        0       0  \n",
       "399       0      0    0      0       0        0       0  \n",
       "\n",
       "[400 rows x 105200 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from termfrequency import collect_vocabulary, get_terms, vectorize, calculate_idfs, vectorize_idf\n",
    "from Normalizer import l2normalize, minmaxnormalize\n",
    "# get term frequency for the tokens\n",
    "\n",
    "doc_terms = {}\n",
    "for doc_id in range(len(X_train)):\n",
    "    doc_terms[doc_id] = get_terms(X_train[doc_id]# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "    \n",
    "# collect the vocabulary from the training set\n",
    "allvocab = collect_vocabulary(X_train)\n",
    "doc_vectors = vectorize(doc_terms,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "                        )\n",
    "doc_idfs = calculate_idfs(allvocab, doc_terms# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "                        )\n",
    "doc_vectors = vectorize_idf(doc_terms, doc_idfs, allvocab)\n",
    "df_Train = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "print(\"done1\")\n",
    "\n",
    "# do the same for the eval set\n",
    "doc_terms_eval = {}\n",
    "for doc_id in range(len(X_eval)):\n",
    "    doc_terms_eval[doc_id] = get_terms(X_eval[doc_id]# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "allvocab = collect_vocabulary(X_eval)\n",
    "doc_vectors = vectorize(doc_terms_eval,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "                        )\n",
    "doc_idfs = calculate_idfs(allvocab, doc_terms_eval# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "                        )\n",
    "doc_vectors = vectorize_idf(doc_terms_eval, doc_idfs, allvocab)\n",
    "df_eval = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "print(\"done2\")\n",
    "\n",
    "# TF-IDF for n-grams training set\n",
    "# doc_terms_ngrams_train = {}\n",
    "# for doc_id in range(len(X_train_ngrams)):\n",
    "#     doc_terms_ngrams_train[doc_id] = get_terms(X_train_ngrams[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(X_train_ngrams)\n",
    "# doc_vectors = vectorize(doc_terms_ngrams_train,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "# )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_ngrams_train# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_ngrams_train, doc_idfs, allvocab)\n",
    "\n",
    "# allvocabstr = ['_'.join(col) for col in allvocab]\n",
    "# df_train_ngrams = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocabstr)\n",
    "\n",
    "# print(\"done3\")\n",
    "\n",
    "# # TF-IDF for n-grams eval set\n",
    "# doc_terms_ngrams_eval = {}\n",
    "# for doc_id in range(len(X_eval_ngrams)):\n",
    "#     doc_terms_ngrams_eval[doc_id] = get_terms(X_eval_ngrams[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(X_eval_ngrams)\n",
    "# doc_vectors = vectorize(doc_terms_ngrams_eval,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "# )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_ngrams_eval# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_ngrams_eval, doc_idfs, allvocab)\n",
    "# allvocabstr = ['_'.join(col) for col in allvocab]\n",
    "# df_eval_ngrams = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocabstr)\n",
    "\n",
    "# df_Train = pd.concat([df_Train, df_train_ngrams], axis=1)\n",
    "# df_eval = pd.concat([df_eval, df_eval_ngrams], axis=1)\n",
    "# print(\"done4\")\n",
    "\n",
    "\n",
    "doc_terms_nounphrsestrain = {}\n",
    "for doc_id in range(len(nounPhrasestrain)):\n",
    "    doc_terms_nounphrsestrain[doc_id] = get_terms(nounPhrasestrain[doc_id]# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "allvocab = collect_vocabulary(nounPhrasestrain)\n",
    "doc_vectors = vectorize(doc_terms_nounphrsestrain,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "                        )\n",
    "doc_idfs = calculate_idfs(allvocab, doc_terms_nounphrsestrain# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "                        )\n",
    "doc_vectors = vectorize_idf(doc_terms_nounphrsestrain, doc_idfs, allvocab)\n",
    "df_nounPhrasestrain = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "print(\"done5\")\n",
    "doc_terms_nounphrseseval = {}\n",
    "for doc_id in range(len(nounPhraseseval)):\n",
    "    doc_terms_nounphrseseval[doc_id] = get_terms(nounPhraseseval[doc_id]# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "allvocab = collect_vocabulary(nounPhraseseval)\n",
    "doc_vectors = vectorize(doc_terms_nounphrseseval,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "                        )\n",
    "doc_idfs = calculate_idfs(allvocab, doc_terms_nounphrseseval# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "                        )\n",
    "doc_vectors = vectorize_idf(doc_terms_nounphrseseval, doc_idfs, allvocab)\n",
    "df_nounPhraseseval = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "print(\"done6\")\n",
    "\n",
    "df_nounPhrasestrain = df_nounPhrasestrain.loc[:, ~df_nounPhrasestrain.columns.isin(df_Train.columns)]\n",
    "df_nounPhraseseval = df_nounPhraseseval.loc[:, ~df_nounPhraseseval.columns.isin(df_eval.columns)]\n",
    "\n",
    "df_Train = pd.concat([df_Train, df_nounPhrasestrain], axis=1)\n",
    "df_eval = pd.concat([df_eval, df_nounPhraseseval], axis=1)\n",
    "\n",
    "\n",
    "# Ensure the DataFrame is in the correct order\n",
    "df_Train = df_Train.sort_index(axis=1)\n",
    "\n",
    "# Ensure the DataFrame is in the correct order\n",
    "df_eval = df_eval.sort_index(axis=1)\n",
    "\n",
    "# Ensure df_eval has the same columns as df_Train\n",
    "# Remove columns not in df_Train\n",
    "df_eval = df_eval.loc[:, df_eval.columns.isin(df_Train.columns)]\n",
    "\n",
    "# Add missing columns to df_eval and set their values to 0\n",
    "missing_cols = [col for col in df_Train.columns if col not in df_eval.columns]\n",
    "missing_df = pd.DataFrame(0, index=df_eval.index, columns=missing_cols)\n",
    "df_eval = pd.concat([df_eval, missing_df], axis=1)\n",
    "\n",
    "# Reorder columns to match df_Train\n",
    "df_eval = df_eval.reindex(columns=df_Train.columns)\n",
    "\n",
    "df_eval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining feature matrices\n",
    "# from PoSTagging import combine_matrices\n",
    "# df_Train = combine_matrices(df_Train, postagstrain)\n",
    "# df_eval = combine_matrices(df_eval, postagseval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.83\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from NaiveBayes import create_classifier\n",
    "\n",
    "#to run on my own model\n",
    "# model = create_classifier(df_Train.values, y_train)\n",
    "# y_pred_eval = model.predict(df_eval.values)\n",
    "# accuracy_test = accuracy_score(y_eval, y_pred_eval)\n",
    "# print(f\"Test Accuracy: {accuracy_test}\")\n",
    "\n",
    "#to run on the sklearn model\n",
    "model = MultinomialNB()\n",
    "model.fit(df_Train, y_train)\n",
    "y_pred_eval = model.predict(df_eval)\n",
    "accuracy_test = accuracy_score(y_eval, y_pred_eval)\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
