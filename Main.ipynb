{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DataSplit import load_data\n",
    "from Tokenizer import tokenize\n",
    "\n",
    "X_train, X_eval, X_test, y_train, y_eval, y_test = load_data()\n",
    "\n",
    "\n",
    "tokenize(X_train) # Basic tokenizer that removes punctuation and lowercases everything\n",
    "tokenize(X_eval)\n",
    "tokenize(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tokenizer import remove_stopwords_and_stem\n",
    "from Tokenizer import remove_stopwords_and_lemmatize\n",
    "from FrequencyCut import cut\n",
    "from PoSTagging import run\n",
    "import numpy as np\n",
    "\n",
    "# edit the stopwords to not remove the word \"not\"\n",
    "\n",
    "for x in range(len(X_train)):\n",
    "    # X_train[x] = remove_stopwords_and_stem(X_train[x])\n",
    "    X_train[x] = remove_stopwords_and_lemmatize(X_train[x])\n",
    "\n",
    "for x in range(len(X_eval)):\n",
    "    # X_eval[x] = remove_stopwords_and_stem(X_eval[x])\n",
    "    X_eval[x] = remove_stopwords_and_lemmatize(X_eval[x])\n",
    "\n",
    "for x in range(len(X_test)):\n",
    "    # X_test[x] = remove_stopwords_and_stem(X_test[x])\n",
    "    X_test[x] = remove_stopwords_and_lemmatize(X_test[x])\n",
    "\n",
    "\n",
    "\n",
    "# X_train = cut(X_train, 0.0001) #uncomment for frequency cut\n",
    "# X_eval = cut(X_eval, 0.0001) #uncomment for frequency cut\n",
    "# X_test = cut(X_test, 0.0001) #uncomment for frequency cut\n",
    "# nounPhrasestrain = run(X_train) #uncomment this for noun phrases\n",
    "# nounPhraseseval = run(X_eval) #uncomment this for noun phrases\n",
    "# nounPhrasestest = run(X_test) #uncomment this for noun phrases\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('folk', 'texaslouisiana'), ('texaslouisiana', 'border'), ('border', 'paramounts'), ('paramounts', 'b'), ('b', 'unit'), ('unit', 'inflicted'), ('inflicted', 'public'), ('public', 'area'), ('area', 'sabine'), ('sabine', 'river'), ('river', 'open'), ('open', 'cotton'), ('cotton', 'country'), ('country', 'deep'), ('deep', 'south'), ('south', 'deep'), ('deep', 'south'), ('south', 'post'), ('post', 'civl'), ('civl', 'cattle'), ('cattle', 'empire'), ('empire', 'theyre'), ('theyre', 'farther'), ('farther', 'west'), ('west', 'texas'), ('texas', 'farther'), ('farther', 'richard'), ('richard', 'dix'), ('dix', 'preston'), ('preston', 'foster'), ('foster', 'ride'), ('ride', 'empirebr'), ('empirebr', 'partner'), ('partner', 'riverboat'), ('riverboat', 'leo'), ('leo', 'carrillo'), ('carrillo', 'theft'), ('theft', 'service'), ('service', 'paying'), ('paying', 'hauling'), ('hauling', 'cattle'), ('cattle', 'cattle'), ('cattle', 'ponderosa'), ('ponderosa', 'ranch'), ('ranch', 'startbr'), ('startbr', 'foster'), ('foster', 'marries'), ('marries', 'dix'), ('dix', 'sister'), ('sister', 'france'), ('france', 'gifford'), ('gifford', 'feud'), ('feud', 'smaller'), ('smaller', 'neighbor'), ('neighbor', 'in'), ('in', 'leo'), ('leo', 'carrillobr'), ('carrillobr', 'anyway'), ('anyway', 'easterner'), ('easterner', 'western'), ('western', 'usually'), ('usually', 'bother'), ('bother', 'geographical'), ('geographical', 'trifle'), ('trifle', 'western'), ('western', 'mill'), ('mill', 'harry'), ('harry', 'sherman'), ('sherman', 'produced'), ('produced', 'hopalong'), ('hopalong', 'cassidy'), ('cassidy', 'western'), ('western', 'paramount'), ('paramount', 'climax'), ('climax', 'blazing'), ('blazing', 'literally'), ('literally', 'gun'), ('gun', 'battle'), ('battle', 'productionbr'), ('productionbr', 'wouldnt'), ('wouldnt', 'western')]\n"
     ]
    }
   ],
   "source": [
    "#Generate n-grams\n",
    "from NGrams import generate_ngrams\n",
    "n = 2\n",
    "X_train_ngrams = generate_ngrams(X_train, n)\n",
    "X_eval_ngrams = generate_ngrams(X_eval, n)\n",
    "X_test_ngrams = generate_ngrams(X_test, n)\n",
    "\n",
    "print (X_train_ngrams[1])\n",
    "\n",
    "\n",
    "# Print the n-grams\n",
    "# for i, ngrams in enumerate(X_train_ngrams):\n",
    "#     print(f\"Sentence {i+1} {n}-grams: {ngrams}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done1\n",
      "done2\n",
      "donetest\n",
      "done3\n",
      "done4\n",
      "done5\n"
     ]
    }
   ],
   "source": [
    "#Feature extraction\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from termfrequency import collect_vocabulary, get_terms, vectorize, calculate_idfs, vectorize_idf\n",
    "# get term frequency for the tokens\n",
    "\n",
    "doc_terms = {}\n",
    "for doc_id in range(len(X_train)):\n",
    "    doc_terms[doc_id] = get_terms(X_train[doc_id]# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "    \n",
    "# collect the vocabulary from the training set\n",
    "allvocab = collect_vocabulary(X_train)\n",
    "doc_vectors = vectorize(doc_terms,allvocab # apply vectorize to the doc_terms and the shared vocabulary allvocab\n",
    "                        )\n",
    "doc_idfs = calculate_idfs(allvocab, doc_terms# apply calculate_idfs to the shared vocabulary allvoccab and to doc_terms\n",
    "                        )\n",
    "doc_vectors = vectorize_idf(doc_terms, doc_idfs, allvocab)\n",
    "df_Train = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "print(\"done1\")\n",
    "\n",
    "# do the same for the eval set\n",
    "doc_terms_eval = {}\n",
    "for doc_id in range(len(X_eval)):\n",
    "    doc_terms_eval[doc_id] = get_terms(X_eval[doc_id]# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "allvocab = collect_vocabulary(X_eval)\n",
    "doc_vectors = vectorize(doc_terms_eval,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "                        )\n",
    "doc_idfs = calculate_idfs(allvocab, doc_terms_eval# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "                        )\n",
    "doc_vectors = vectorize_idf(doc_terms_eval, doc_idfs, allvocab)\n",
    "df_eval = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "print(\"done2\")\n",
    "\n",
    "doc_terms_test = {}\n",
    "for doc_id in range(len(X_test)):\n",
    "    doc_terms_test[doc_id] = get_terms(X_test[doc_id]# apply to the content of the document with id doc_id\n",
    "                                  )\n",
    "allvocab = collect_vocabulary(X_test)\n",
    "doc_vectors = vectorize(doc_terms_test,allvocab) # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "doc_idfs = calculate_idfs(allvocab, doc_terms_test) # apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "doc_vectors = vectorize_idf(doc_terms_test, doc_idfs, allvocab)\n",
    "df_test = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "print(\"donetest\")\n",
    "\n",
    "# TF-IDF for n-grams training set\n",
    "# doc_terms_ngrams_train = {}\n",
    "# for doc_id in range(len(X_train_ngrams)):\n",
    "#     doc_terms_ngrams_train[doc_id] = get_terms(X_train_ngrams[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(X_train_ngrams)\n",
    "# doc_vectors = vectorize(doc_terms_ngrams_train,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "# )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_ngrams_train# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_ngrams_train, doc_idfs, allvocab)\n",
    "\n",
    "# allvocabstr = ['_'.join(col) for col in allvocab]\n",
    "# df_train_ngrams = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocabstr)\n",
    "\n",
    "# print(\"done3\")\n",
    "\n",
    "# # TF-IDF for n-grams eval set\n",
    "# doc_terms_ngrams_eval = {}\n",
    "# for doc_id in range(len(X_eval_ngrams)):\n",
    "#     doc_terms_ngrams_eval[doc_id] = get_terms(X_eval_ngrams[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(X_eval_ngrams)\n",
    "# doc_vectors = vectorize(doc_terms_ngrams_eval,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "# )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_ngrams_eval# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_ngrams_eval, doc_idfs, allvocab)\n",
    "# allvocabstr = ['_'.join(col) for col in allvocab]\n",
    "# df_eval_ngrams = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocabstr)\n",
    "\n",
    "# print(\"done4\")\n",
    "\n",
    "# # TF-IDF for n-grams test set\n",
    "# doc_terms_ngrams_test = {}\n",
    "# for doc_id in range(len(X_test_ngrams)):\n",
    "#     doc_terms_ngrams_test[doc_id] = get_terms(X_test_ngrams[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(X_test_ngrams)\n",
    "# doc_vectors = vectorize(doc_terms_ngrams_test,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "# )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_ngrams_test# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_ngrams_test, doc_idfs, allvocab)\n",
    "# allvocabstr = ['_'.join(col) for col in allvocab]\n",
    "# df_test_ngrams = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocabstr)\n",
    "\n",
    "# print(\"done5\")\n",
    "\n",
    "\n",
    "# doc_terms_nounphrsestrain = {}\n",
    "# for doc_id in range(len(nounPhrasestrain)):\n",
    "#     doc_terms_nounphrsestrain[doc_id] = get_terms(nounPhrasestrain[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(nounPhrasestrain)\n",
    "# doc_vectors = vectorize(doc_terms_nounphrsestrain,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "#                         )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_nounphrsestrain# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_nounphrsestrain, doc_idfs, allvocab)\n",
    "# df_nounPhrasestrain = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "# print(\"done6\")\n",
    "# doc_terms_nounphrseseval = {}\n",
    "# for doc_id in range(len(nounPhraseseval)):\n",
    "#     doc_terms_nounphrseseval[doc_id] = get_terms(nounPhraseseval[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(nounPhraseseval)\n",
    "# doc_vectors = vectorize(doc_terms_nounphrseseval,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "#                         )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_nounphrseseval# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_nounphrseseval, doc_idfs, allvocab)\n",
    "# df_nounPhraseseval = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "# print(\"done7\")\n",
    "\n",
    "# doc_terms_nounphrasestest = {}\n",
    "# for doc_id in range(len(nounPhrasestest)):\n",
    "#     doc_terms_nounphrasestest[doc_id] = get_terms(nounPhrasestest[doc_id]# apply to the content of the document with id doc_id\n",
    "#                                   )\n",
    "# allvocab = collect_vocabulary(nounPhrasestest)\n",
    "# doc_vectors = vectorize(doc_terms_nounphrasestest,allvocab # apply vectorize to the doc_terms and the shared vocabulary all_terms\n",
    "#                         )\n",
    "# doc_idfs = calculate_idfs(allvocab, doc_terms_nounphrasestest# apply calculate_idfs to the shared vocabulary all_terms and to doc_terms\n",
    "#                         )\n",
    "# doc_vectors = vectorize_idf(doc_terms_nounphrasestest, doc_idfs, allvocab)\n",
    "# df_nounPhrasestest = pd.DataFrame.from_dict(doc_vectors, orient='index', columns=allvocab)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Normalizer import l2normalize, minmaxnormalize\n",
    "def formatting(df,df2):\n",
    "    # Ensure the DataFrame is in the correct order\n",
    "    df = df.sort_index(axis=1)\n",
    "\n",
    "    # Ensure the DataFrame is in the correct order\n",
    "    df2 = df2.sort_index(axis=1)\n",
    "\n",
    "    # Ensure df_eval has the same columns as df_Train\n",
    "    # Remove columns not in df_Train\n",
    "    df2 = df2.loc[:, df2.columns.isin(df.columns)]\n",
    "\n",
    "    # Add missing columns to df_eval and set their values to 0\n",
    "    missing_cols = [col for col in df.columns if col not in df2.columns]\n",
    "    missing_df = pd.DataFrame(0, index=df2.index, columns=missing_cols)\n",
    "    df2 = pd.concat([df2, missing_df], axis=1)\n",
    "\n",
    "    # Reorder columns to match df_Train\n",
    "    df2 = df2.reindex(columns=df.columns)\n",
    "\n",
    "    return df, df2\n",
    "\n",
    "def add_prefix_to_columns(df, prefix):\n",
    "    df.columns = [f\"{prefix}_{col}\" for col in df.columns]\n",
    "    return df\n",
    "\n",
    "df_Train = l2normalize(df_Train)\n",
    "df_eval = l2normalize(df_eval)\n",
    "df_test = l2normalize(df_test)\n",
    "# df_nounPhrasestrain = l2normalize(df_nounPhrasestrain)\n",
    "# df_nounPhraseseval = l2normalize(df_nounPhraseseval)\n",
    "# df_nounPhrasestest = l2normalize(df_nounPhrasestest)\n",
    "# df_train_ngrams = l2normalize(df_train_ngrams)\n",
    "# df_eval_ngrams = l2normalize(df_eval_ngrams)\n",
    "# df_test_ngrams = l2normalize(df_test_ngrams)\n",
    "# add_prefix_to_columns(df_nounPhrasestrain, \"NP\")\n",
    "# add_prefix_to_columns(df_nounPhraseseval, \"NP\")\n",
    "# add_prefix_to_columns(df_nounPhrasestest, \"NP\")\n",
    "# add_prefix_to_columns(df_train_ngrams, \"NG\")\n",
    "# add_prefix_to_columns(df_eval_ngrams, \"NG\")\n",
    "# add_prefix_to_columns(df_test_ngrams, \"NG\")\n",
    "# df_Train = pd.concat([df_Train, df_nounPhrasestrain], axis=1)\n",
    "# df_eval = pd.concat([df_eval, df_nounPhraseseval], axis=1)\n",
    "# df_test = pd.concat([df_test, df_nounPhrasestest], axis=1)\n",
    "# df_Train = pd.concat([df_Train, df_train_ngrams], axis=1)\n",
    "# df_eval = pd.concat([df_eval, df_eval_ngrams], axis=1)\n",
    "# df_test = pd.concat([df_test, df_test_ngrams], axis=1)\n",
    "\n",
    "df_Train, df_eval = formatting(df_Train, df_eval)\n",
    "df_Train, df_test = formatting(df_Train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval Accuracy: 0.8575\n",
      "Test Accuracy: 0.8325\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from NaiveBayes import create_classifier\n",
    "\n",
    "#to run on my own model\n",
    "# model = create_classifier(df_Train.values, y_train)\n",
    "# y_pred_eval = model.predict(df_test.values)\n",
    "# accuracy_test = accuracy_score(y_test, y_pred_eval)\n",
    "# print(f\"Test Accuracy: {accuracy_test}\")\n",
    "\n",
    "#to run on the sklearn model\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(df_Train, y_train)\n",
    "y_pred_eval = model.predict(df_eval)\n",
    "accuracy_eval = accuracy_score(y_eval, y_pred_eval)\n",
    "y_pred_test = model.predict(df_test)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Eval Accuracy: {accuracy_eval}\")\n",
    "print(f\"Test Accuracy: {accuracy_test}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
